# Linear Regression â€“ From Scratch & Analytical Study

## ğŸ“Œ Overview
This module focuses on understanding Linear Regression from both a mathematical and implementation perspective.  
The objective is to move beyond library usage and deeply understand how optimization and closed-form solutions work.

The following implementations are included:
- Linear Regression using Gradient Descent (from scratch)
- Linear Regression using Ordinary Least Squares (Normal Equation)
- Comparison between both approaches

---

## ğŸ§  Theory Summary

Linear Regression models the relationship between independent variables (X) and a dependent variable (y) using a linear equation:

y = XÎ² + Îµ

Where:
- Î² represents model parameters (weights)
- Îµ represents error

The goal is to minimize the Mean Squared Error (MSE):

J(Î²) = (1/m) Î£ (y_pred - y_actual)Â²

Two approaches were implemented:

### 1ï¸âƒ£ Gradient Descent (Iterative Optimization)
- Uses learning rate (Î±)
- Updates weights iteratively
- Tracks cost reduction over iterations

### 2ï¸âƒ£ Ordinary Least Squares (Closed-Form Solution)
Uses the Normal Equation:

Î² = (Xáµ€X)â»Â¹ Xáµ€ y

- No learning rate
- No iterations
- Direct analytical solution

---

## ğŸ›  Implementation Details

### Files Included
- `linear_regression_gd.py` â†’ Gradient Descent implementation
- `linear_regression_ols.py` â†’ Normal Equation implementation
- `notebook.ipynb` â†’ Data preprocessing, visualization, and comparison

### Key Components Implemented
- Cost function (Mean Squared Error)
- Weight initialization
- Bias handling
- Learning rate tuning
- Cost history tracking
- Model evaluation
- Visualization of predictions

---

## ğŸ“Š Model Evaluation

Evaluation metrics used:
- Mean Squared Error (MSE)
- RÂ² Score
- Predicted vs Actual Plot
- Cost vs Iterations Plot (for GD)

---

## ğŸ” Observations & Insights

- Gradient Descent converges gradually depending on learning rate.
- Very high learning rate causes divergence.
- OLS provides an exact solution but can be computationally expensive for large feature sets.
- Both methods produce similar coefficients when GD converges properly.

---

## ğŸš€ Key Learning Outcomes

- Deep understanding of optimization in ML
- Practical implementation of gradient-based learning
- Difference between iterative and analytical solutions
- Importance of feature scaling in convergence

---

## ğŸ“¦ Dependencies

See `requirements.txt` in the root directory.
